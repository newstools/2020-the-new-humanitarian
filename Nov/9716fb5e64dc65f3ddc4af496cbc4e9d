When I started my career, I got a foot in the door by making bar graphs. Way back then, (let’s just say it was the pre-Windows era), bosses were super-impressed. Things haven’t changed much. People still love graphs and maps. Bosses like them. Donors like them. In this often depressing world of humanitarian action, data glitters. But the way the world has gone, it’s time for a talk about data responsibilities. Misinformation and disinformation are running wild. Data can be – is being – weaponised. What I do for work these days still involves bar graphs from time to time. But it’s mainly words, and journalism. Our purpose at The New Humanitarian is to inform decision-makers. We don't make recommendations, but we try to serve up the ingredients of better decision-making. We’re one of very few news organisations that specialise in this field. One of our selling points is that we don't dumb it down: A bit less sexy and clickable than some perhaps, but we’re not going to give readers a caricature. Nobody can claim absolute neutrality in journalism, but we try to be conscious of where our biases might be, and not to indulge in the worst habits that the media and aid industries can be guilty of. These are fairly conventional journalistic principles: not just a commitment to accuracy, but taking care in sourcing, a sense of balance, and an appreciation of the wider context. Given how much of the material about crises and emergency relief is now driven by numbers, maps, and graphs, those values really matter for the way data is presented too. You see, your maps, your surveys, your COVID-19 graphs, your cool infographics, they're not neutral. Your facts all come from somewhere, loaded with history, with freight, with inevitable errors and estimations. When you put them together, you too are committing acts of journalism, acts of analysis. Pouring into that data lake are your institutional politics, as well as a range of economic, social, and cultural undercurrents. In today’s multiverse of crises, the information manager and the data engineer also have to own the influence they wield: As soon as you start building databases, putting out a questionnaire, coding a model, you’re taking a position. Here’s one example from my own past. I was working in a country in a civil war, and humanitarian agencies were trying to deliver aid on both sides. My headquarters was demanding numbers on how many people we were helping across the front line. When we produced the bar graph, HQ were not satisfied. The government side looked like they were getting the lion’s share (they were). We ended up giving them a different graph, based on the number of villages instead of the numbers of people. Comparing the number of villages told a much “better” story: they were smaller, so the same amount of people showed as a bigger number on the rebel side. It was misleading, disingenuous. But HQ was happier. Presumably a donor got a little bit of reassurance hearing what they wanted to hear. My colleague, the maps and data guy, and I just felt a bit dirty, really. This is the kind of small violation of the truth, perversion, distortion, you may recognise. You know what the difference is between an honest presentation of the data and a manipulated presentation. Who better to call it out than you? Try not to be the humanitarian tech who looks back in regret, saying: “I wish I had tried harder to put the brakes on.” Time for a fresh example: British statisticians funded from the UK aid budget are trying to train their algorithms to count the number of cows in South Sudan automatically from satellite imagery, as current estimates are out of date and unreliable. Smart, useful, and harmless, on the face of it. But it wouldn’t be a total surprise to read six months from now that the project had run into trouble. Billions of dollars of wealth are on the hoof, and cows have profound cultural significance. The terrain and conflict dynamics are daunting. To their credit, the data scientists flag at least one major risk: publishing the location of the herds would be an invitation to cattle raiders. But who knows what else could go wrong? My colleague, the maps and data guy, and I just felt a bit dirty, really. This is a point well made by Nathaniel Raymond of Yale University: How can you talk about doing no harm if you don't even know what the harm might be? This reflection calls for some humility in the humanitarian sector, and restraint. It’s another reason for data professionals to say: “Have we even thought of all the possible ways this could go wrong and, if we haven't, who could help us think it through?” There are a plethora of guidelines on data in humanitarian settings: the Principles of Digital Development; the Harvard Signal Code; UN OCHA’s eight-page report on ethical traps. Meanwhile, the ICRC is already onto the second edition of its Handbook on Data Protection in Humanitarian Action: 312 pages, ladies and gentlemen. These are all worth your attention. It’s notable that aid agencies and donors have stepped up their attention to data protection, and “responsible data”, especially after lapses and controversies were exposed. The leadership of the aid agencies carry the formal responsibility for what they do with data, of course, and things are improving. But it’s too important to leave to slow-moving boards and executives. We rely on the integrity of individual journalists, as well as their editors and proprietors not to mislead us. Given how poorly understood and monitored the realm of data can be, there’s a role here too for the individuals who actually deal with the data to keep their institutions honest. Don't wait for the fancy advisory committee. Don't get stuck on the 300-page manual. You can start any time, just by telling your boss: “I think we need to maybe have a rethink.” We need to do this stuff better. You're the first line of defence. The values in the data set need to reflect your values. (This article is based on a keynote presentation at the GeOnG conference on 2 November 2020.)